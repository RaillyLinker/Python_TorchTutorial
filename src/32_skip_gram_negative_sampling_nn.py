"""
[네거티브 샘플링(Negative Sampling)]
- 대체적으로 Word2Vec를 사용한다고 하면 SGNS(Skip-Gram with Negative Sampling)을 사용합니다.

    위에서 정리한 Word2Vec 모델에는 한 가지 문제점이 있습니다.
    바로 속도입니다.
    Word2Vec의 마지막 단계를 주목해봅시다.

    출력층에 있는 소프트맥스 함수는 단어 집합 크기의 벡터 내의 모든 값을 0과 1사이의 값이면서 모두 더하면 1이 되도록 바꾸는 작업을 수행합니다.
    그리고 이에 대한 오차를 구하고 모든 단어에 대한 임베딩을 조정합니다.
    그 단어가 중심 단어나 주변 단어와 전혀 상관없는 단어라도 마찬가지 입니다.
    그런데 만약 단어 집합의 크기가 수백만에 달한다면 이 작업은 굉장히 무거운 작업입니다. (즉, 원 핫 벡터의 크기가 커질수록 연산량이 커집니다.)

    여기서 중요한 건 Word2Vec이 모든 단어 집합에 대해서 소프트맥스 함수를 수행하고,
    역전파를 수행하므로 주변 단어와 상관 없는 모든 단어까지의 워드 임베딩 조정 작업을 수행한다는 겁니다.
    만약 마지막 단계에서 '강아지'와 '고양이'와 같은 단어에 집중하고 있다면,
    Word2Vec은 사실 '돈가스'나 '컴퓨터'와 같은 연관 관계가 없는 수많은 단어의 임베딩을 조정할 필요가 없습니다.

    이를 조금 더 효율적으로 할 수 있는 방법이 없을까요?
    전체 단어 집합이 아니라 일부 단어 집합에 대해서만 고려하면 안 될까요?
    이렇게 일부 단어 집합을 만들어봅시다. '강아지', '고양이', '애교'와 같은 주변 단어들을 가져옵니다.
    그리고 여기에 '돈가스', '컴퓨터', '회의실'과 같은 랜덤으로 선택된 주변 단어가 아닌 상관없는 단어들을 일부만 갖고옵니다.
    이렇게 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 바꿔버리는 겁니다.
    즉, Word2Vec은 주변 단어들을 긍정(positive)으로 두고 랜덤으로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행합니다.

    이는 기존의 다중 클래스 분류 문제를 이진 분류 문제로 바꾸면서도 연산량에 있어서 훨씬 효율적입니다.

    다음 챕터에서 영어와 한국어 훈련 데이터에 대해서 Word2Vec 모델을 훈련시키는 실습을 진행해보겠습니다.
"""

# todo
