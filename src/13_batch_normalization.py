import torch
from torch import nn

"""
[배치 정규화 샘플]
(장점)
학습 속도 향상: 배치 정규화는 학습 속도를 향상시킬 수 있습니다. 그 이유는 각 레이어의 입력이 정규화되어 있기 때문에 학습 속도가 빨라지고, 따라서 더 빠르게 수렴할 수 있습니다.
초기화에 대한 덜 민감: 배치 정규화는 가중치 초기화에 대해 덜 민감합니다. 가중치 초기화가 잘못되었을 때도 학습이 잘 진행될 수 있습니다.
규제 효과: 배치 정규화는 일종의 규제(regularization) 효과를 가지며, 과적합을 방지하는 데 도움을 줄 수 있습니다.
더 큰 학습률 사용: 배치 정규화를 사용하면 더 큰 학습률을 사용할 수 있습니다. 이는 학습 과정을 가속화하고 더 빨리 수렴할 수 있게 합니다.
활성화 함수의 출력 분포를 조정: 배치 정규화는 활성화 함수의 출력 분포를 조정하여 기울기 소실(vanishing gradient) 문제를 완화합니다. 특히 시그모이드 함수와 같은 활성화 함수의 출력이 0 또는 1에 가까워질 때 발생하는 기울기 소실 문제를 완화하는 데 도움이 됩니다.

(배치 정규화 적용 시점)
배치 정규화는 보통 신경망의 각 레이어 이후에 배치됩니다. 주로 활성화 함수(예: ReLU) 이전이나 후에 배치 정규화를 적용하는 것이 일반적입니다.
선형 변환 이후: 대부분의 경우, 선형 변환(예: 완전 연결 레이어나 합성곱 레이어) 이후에 배치 정규화를 적용합니다. 이는 활성화 함수 이전에 배치 정규화를 적용하는 것이 일반적입니다.
활성화 함수 이전: 일반적으로 배치 정규화는 활성화 함수(예: ReLU) 이전에 배치됩니다. 이렇게 하면 활성화 함수의 입력 분포가 정규화되어 기울기 소실 문제를 완화할 수 있습니다.
완전 연결 레이어와 합성곱 레이어 사이: 배치 정규화는 보통 완전 연결 레이어나 합성곱 레이어 이후에 적용됩니다. 이 위치에서 배치 정규화를 사용하면 각 레이어의 출력이 다음 레이어로 전달되기 전에 정규화됩니다.
출력에 가까운 층들: 신경망의 출력에 가까운 층들에서도 배치 정규화를 사용하는 것이 일반적입니다. 이는 네트워크의 출력을 정규화하여 안정적인 예측을 만들어내도록 돕습니다.
요약하면, 배치 정규화는 보통 각 레이어의 선형 변환 이후 또는 활성화 함수 이전에 배치됩니다.
"""

x = torch.FloatTensor(
    [
        [-0.6577, -0.5797, 0.6360],
        [0.7392, 0.2145, 1.523],
        [0.2432, 0.5662, 0.322]
    ]
)

# 배치 정규화 실행
# 데이터의 평균을 0으로, 분산을 1로 만듭니다.
# ((x - E(X)) / root(var(X) + e)) * r + b
# 값에 평균을 빼고, 0 이 되는걸 방지하는 극소값을 더한 분산값에 루트를 씌워서 나누고 매개변수 r 과 b 를 계산해 줍니다.
print(x[0].shape[0])
norm_tensor = nn.BatchNorm1d(torch.Size([3])[0])(x)

print(norm_tensor)
