"""
[하위 단어 토크나이징]
인간이 이해하는 방식으로 자연어 데이터 분석을 하기 위해서는 공백 단위 텍스트 분리보다는 형태소 단위의 텍스트 분리가 효과적일 것입니다.
하지만 언어라는 것은 형태소일지라도 시간이 지남에 따라 생성, 소멸, 수정의 변화가 있을 수 있습니다.
또한 휴먼 에러로 인하여 오탈자로 인한 오류가 있을 수도 있죠.
즉, 단순한 형태소 분석은 신조어, 외래어, 전문용어, 고유어 등을 처리할 때 약점을 보입니다.
이를 해결하기 위한 방법 중 하나는 분리된 단어를 더 작은 단위로 쪼개는 방법이 있습니다.
그냥 쪼개는 것이 아니라 빈번하게 사용되는 하위 단어의 조합으로 쪼개는 알고리즘입니다.
이것을 하위 단어 토큰화라고 합니다.
하위 단어 토큰화에는 두가지 방식이 존재합니다.
"""

"""
[바이트페어 인코딩]
바이트페어 인코딩 (Byte Pair Encoding : BPE) 란, 다이어그램 코딩(Diagram Coding) 이라고도 합니다.
데이터 압축 기술에서 시작된 방식이지만, 후에 자연어 처리(NLP) 분야, 특히 텍스트 또는 언어 모델링에서 토큰화(tokenization) 기법으로 널리 사용되고 있습니다.
이 방식의 기본 아이디어는 말뭉치(corpus) 내에서 가장 빈번하게 등장하는 바이트 쌍(혹은 문자 쌍)을 반복적으로 합치면서 새로운 단어 또는 토큰을 생성하는 것입니다.
이렇게 함으로써, 모델이 처리해야 할 토큰의 수를 줄이고, 언어의 다양한 변형(variation)을 더 잘 핸들링할 수 있게 됩니다.

이 방식을 통해 모델은 미처 학습하지 못한 단어들도 서브워드 또는 문자의 조합으로 표현하고 이해할 수 있게 됩니다.
예를 들어, 'un-'와 같은 접두사나 '-ing'과 같은 접미사를 포함한 단어들은 학습 과정에서 직접 마주치지 않았더라도,
이들의 구성 요소를 통해 의미를 유추할 수 있습니다.
BPE는 따라서 언어 모델이 새로운 단어나 희귀 단어에 대해 더 나은 일반화(generalization) 능력을 가지도록 돕습니다.
"""

# todo https://wikidocs.net/22592

"""
[워드피스]
Wordpiece 토크나이저는 바이트 페어 인코딩 토크나이저와 듀사한 방법으로 학습되지만,
빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합합니다.

"""

# todo
