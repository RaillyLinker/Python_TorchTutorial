import torch
from torch import nn

"""
[배치 정규화 (Batch Normalization)]
- 배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만듭니다.
- 내부 공변량 변화(Internal Covariate Shift)
    배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있습니다. 
    내부 공변량 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말합니다. 
    이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 
    현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생합니다. 
    배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장합니다.
    공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.
    내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미합니다.
- 배치 정규화(Batch Normalization)는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말합니다. 
    배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행됩니다. 
    배치 정규화를 요약하면 다음과 같습니다. 
    입력에 대해 평균을 0으로 만들고, 정규화를 합니다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행합니다. 
    학습시 사용되는 미니배치의 평균과 분산을 각각 구하여, 입력되는 데이터를,
    x - 평균 / sqrt(분산 + 극소값) 이런 수식으로 계산하여 변환하면 됩니다.
- 배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 
    테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 합니다.
    학습 데이터의 수가 충분히 크고, 학습 데이터가 실제 데이터 형태를 반영한다고 가정하면, 실제 파라미터(평균과 분포)는 정확히 일치할 것이기 때문이죠.
- 배치 정규화의 효과는 검증되었지만, 내부 공변량 변화때문은 아니라는 논문도 있습니다. : https://arxiv.org/pdf/1805.11604.pdf

(장점)
- 학습 속도 향상: 
    배치 정규화는 학습 속도를 향상시킬 수 있습니다. 
    그 이유는 각 레이어의 입력이 정규화되어 있기 때문에 학습 속도가 빨라지고, 따라서 더 빠르게 수렴할 수 있습니다.
- 초기화에 대한 덜 민감: 
    배치 정규화는 가중치 초기화에 대해 덜 민감합니다. 
    가중치 초기화가 잘못되었을 때도 학습이 잘 진행될 수 있습니다.
- 규제 효과: 
    배치 정규화는 일종의 규제(regularization) 효과를 가지며, 
    과적합을 방지하는 데 도움을 줄 수 있습니다.
- 더 큰 학습률 사용: 
    배치 정규화를 사용하면 더 큰 학습률을 사용할 수 있습니다. 
    이는 학습 과정을 가속화하고 더 빨리 수렴할 수 있게 합니다.
- 활성화 함수의 출력 분포를 조정: 
    배치 정규화는 활성화 함수의 출력 분포를 조정하여 기울기 소실(vanishing gradient) 문제를 완화합니다. 
    특히 시그모이드 함수와 같은 활성화 함수의 출력이 0 또는 1에 가까워질 때 발생하는 기울기 소실 문제를 완화하는 데 도움이 됩니다.

(단점)
- 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려집니다. 
    그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요합니다.
- 미니 배치 크기에 의존적입니다.
    배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있습니다. 
    단적으로 배치 크기를 1로 하게되면 분산은 0이 됩니다. 
    작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있습니다. 
    배치 정규화를 적용할때는 작은 미니 배치보다는 크기가 어느정도 되는 미니 배치에서 하는 것이 좋습니다. (실제 데이터의 평균과 분산을 충분히 반영할 정도)
    이처럼 배치 정규화는 배치 크기에 의존적인 면이 있습니다.
- RNN 류의 순환 신경망 모델에 적용하기 어렵습니다.
    동일한 레이어의 출력값이 다음 계산의 입력값으로 사용되기를 N 번 실행되는 RNN은 각 시점(time step)마다 다른 통계치를 가지기에
    성능이 안 좋다고 합니다. (이외의 이유도 존재한다고 합니다.)
    이때는 층 정규화(Layer Normalization) 라는 방식을 사용한다고 합니다.

(배치 정규화 적용 시점)
- 배치 정규화는 보통 신경망의 각 레이어 이후에 배치됩니다. 
    주로 활성화 함수(예: ReLU) 이전이나 후에 배치 정규화를 적용하는 것이 일반적입니다.
- 선형 변환 이후: 
    대부분의 경우, 선형 변환(예: 완전 연결 레이어나 합성곱 레이어) 이후에 배치 정규화를 적용합니다. 
    이는 활성화 함수 이전에 배치 정규화를 적용하는 것이 일반적입니다.
- 활성화 함수 이전: 
    일반적으로 배치 정규화는 활성화 함수(예: ReLU) 이전에 배치됩니다. 
    이렇게 하면 활성화 함수의 입력 분포가 정규화되어 기울기 소실 문제를 완화할 수 있습니다.
- 완전 연결 레이어와 합성곱 레이어 사이: 
    배치 정규화는 보통 완전 연결 레이어나 합성곱 레이어 이후에 적용됩니다. 
    이 위치에서 배치 정규화를 사용하면 각 레이어의 출력이 다음 레이어로 전달되기 전에 정규화됩니다.
- 출력에 가까운 층들: 
    신경망의 출력에 가까운 층들에서도 배치 정규화를 사용하는 것이 일반적입니다. 
    이는 네트워크의 출력을 정규화하여 안정적인 예측을 만들어내도록 돕습니다.
- 요약하면, 배치 정규화는 보통 각 레이어의 선형 변환 이후 또는 활성화 함수 이전에 배치됩니다.
"""


# BatchNorm 구현
# 데이터의 평균을 0으로, 표준 편차를 1로 만듭니다. (데이터 분포를 표준 분포로 변경)
class CustomBatchNorm1d(nn.Module):
    def __init__(self, num_features):
        super(CustomBatchNorm1d, self).__init__()
        # 가중치 및 편향 파라미터 생성
        self.gamma = nn.Parameter(torch.ones(num_features))  # scale parameter (학습 가능한 파라미터)
        self.beta = nn.Parameter(torch.zeros(num_features))  # shift parameter (학습 가능한 파라미터)

    def forward(self, model_in):
        # 배치 평균과 배치 분산 계산
        batch_mean = model_in.mean(dim=0)
        batch_var = model_in.var(dim=0, unbiased=False)

        # 배치 정규화 수행
        normalized_x = (model_in - batch_mean) / torch.sqrt(batch_var + 1e-5)

        # Scale 및 Shift 적용
        scaled_x = normalized_x * self.gamma + self.beta
        return scaled_x


# 배치 정규화 실행
x = torch.FloatTensor(
    [
        [-0.6577, -0.5797, 0.6360],
        [0.7392, 0.2145, 1.523],
        [0.2432, 0.5662, 0.322]
    ]
)

feature_size = x[0].shape[0]
print(feature_size)

# 커스텀 배치 정규화 실행
norm_tensor = CustomBatchNorm1d(feature_size)(x)
print(norm_tensor)

# 제공 배치 정규화 실행
norm_tensor = nn.BatchNorm1d(feature_size)(x)
print(norm_tensor)


# 적용 예시
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.bn1 = nn.BatchNorm1d(512)  # 선형 변환 이후, 활성화 이전 적용
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(512, 10)

    def forward(self, model_in):
        model_out = model_in.view(-1, 28 * 28)
        model_out = self.fc1(model_out)
        model_out = self.bn1(model_out)
        model_out = self.relu1(model_out)
        model_out = self.fc2(model_out)
        return model_out
